{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度与梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度的定义与性质\n",
    "在二元函数的情形，设函数$f(x,y)$在平面区域$D$内具有一阶连续偏导数，则对于每一点$P_0(x_0,y_0)\\in D$，都可以定出一个向量：\n",
    "$$f_x(x_0,y_0)i+f_y(x_0,y_0)j,$$\n",
    "这向量称为函数$f(x,y)$在点$P_0(x_0,y_0)$的**梯度**，记作**grand** $f(x,y)$或$\\bigtriangledown f(x_0,y_0)$，即：\n",
    "$$\n",
    "\\displaystyle grad\\,f(x_0,y_0)=\\bigtriangledown f(x_0,y_0)=f_x(x_0,y_0)i+f_y(x_0,y_0)j\n",
    "$$\n",
    "如果函数$f(x,y)$在点$P_0(x_0,y_0)$可微分，$e_l=(cos\\alpha,cos\\beta)$是与方向$l$同向的单位向量，则：\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\displaystyle\\partial f}{\\displaystyle\\partial l}\\bracevert{x_0,y_0}\n",
    "& =f_x(x_0,y_0)cos\\alpha+f_y(x_0,y_0)cos\\beta \\\\\n",
    "& = \\text{grad}\\, f(x_0,y_0)\\cdot e_l \\\\\n",
    "& = \\vert \\text{grad}\\, f(x_0,y_0)\\vert cos\\theta\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中$\\theta$是$\\text{grad}\\, f(x_0,y_0)$与$e_l$的夹角。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 当$\\theta=0$时，即$\\text{grad}\\, f(x_0,y_0)$与$e_l$的方向相同时，函数$f(x,y)$增加最快。此时，函数在这个方向的方向导数达到最大值，这个最大值就是梯度$\\text{grad}\\, f(x_0,y_0)$的模，即\n",
    "$$\\frac{\\displaystyle\\partial f}{\\displaystyle\\partial l}\\bracevert{x_0,y_0}=\\vert \\text{grad}\\, f(x_0,y_0)\\vert$$\n",
    "这个结果也表示：  \n",
    "函数$f(x,y)$在一个点的梯度**grad**$f(x,y)$是这样一个向量，它的方向是函数在这点的方向导数取得最大值的方向，它的模就等于方向导数的最大值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 当$\\theta=\\pi$时，即$\\text{grad}\\, f(x_0,y_0)$与$e_l$的方向相反时，函数$f(x,y)$减少最快，函数在这个方向的方向导数达到最小值，即：\n",
    "$$\\frac{\\displaystyle\\partial f}{\\displaystyle\\partial l}\\bracevert{x_0,y_0}=-\\vert \\text{grad}\\, f(x_0,y_0)\\vert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 当$\\theta=\\pi/2$时，即$\\text{grad}\\, f(x_0,y_0)$与$e_l$的方向正交时，函数的变化率为0，即：\n",
    "$$\\frac{\\displaystyle\\partial f}{\\displaystyle\\partial l}\\bracevert{x_0,y_0}=\\vert \\text{grad}\\, f(x_0,y_0)\\vert\\cos\\theta=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量梯度下降法BGD($batch\\,gradient\\,descent$)\n",
    "设有线性模型为：\n",
    "$$h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2$$\n",
    "这里$x_j$是自变量，$\\theta_j$是参数（线性回归里面的待定参数，也叫做权值）\n",
    "为了简化符号，假设有n个变量，把线性模型改写成：\n",
    "$$h(x)=\\sum_{j=0}^n\\theta_jx_j=\\theta^Tx$$\n",
    "假设有$m$组样本，为了训练样本需要用来误差函数：\n",
    "$$J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2$$\n",
    "假设选定了一组初始化的参数$\\theta$，为了最小化误差函数$J(\\theta)$而训练样本，迭代更新参数$\\theta$。每次迭代，都对权值$\\theta$进行更新，以找到令$J(\\theta)$达到最小时的权值$\\theta$。使用梯度下降算法，即：\n",
    "$$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次迭代，都把所有权值更新一遍。$\\alpha$是指学习速率(learning rate)。下面先求$J(\\theta)$的偏导：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n",
    "& = \\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2}(h_{\\theta}(x)-y)^2 \\\\\n",
    "& = 2\\cdot\\frac{1}{2}(h_{\\theta}(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(h_{\\theta}(x)-y) \\\\\n",
    "& = (h_{\\theta}(x)-y)\\cdot\\frac{\\partial}{\\partial\\theta_j}(\\sum_{i=0}^n\\theta_ix_i-y) \\\\\n",
    "& = (h_{\\theta}(x)-y)x_j\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即可求得，每次迭代的更新权值函数：\n",
    "$$\\theta_j:=\\theta_j+\\alpha\\cdot(y^{(i)}-h_{\\theta}(x^{(i)})x_j^{(i)}$$\n",
    "这个方法叫做**LMS**($least \\,mean\\, squares\\text{最小二乘}$)更新法则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面只是一次迭代，即对于单组样本（但是是多个自变量，多个权值）进行运算，更新所有权值。如果一次训练m组样本，那么训练函数则可以写成：\n",
    "$$\\begin{aligned}\n",
    "& \\text{Repeat until convergence\\{}\\\\\n",
    "& \\,\\,\\,\\,\\theta_j:=\\theta_j+\\alpha\\cdot\\sum_{i=0}^m(y^{(i)}-h_{\\theta}(x^{(i)})x_j^{(i)},(\\text{for every j})\\\\\n",
    "& \\text{\\}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "这叫做**批量梯度下降法**($batch\\,gradient\\,descent$)。但如果$m$很大，则会造成训练速度慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降法SGD($stochastic\\,gradient\\,descent$)\n",
    "由于在样本数目$m$很大时，批量梯度下降法的数据量庞大，所有有一种可替换的随机梯度下降法SGD。随机梯度下降法每次迭代只对一组样本进行运算以更新权值，而随机梯度下降法可以很快地收敛误差函数，很快使得权值$\\theta$得到最优解。  \n",
    "写成伪代码的形式：\n",
    "$$\\begin{aligned}\n",
    "& \\text{Loop}\\,\\{ \\\\\n",
    "& \\,\\text{for i=1 to m\\{}\\\\\n",
    "& \\,\\,\\,\\,\\theta_j:=\\theta_j+\\alpha\\cdot(y^{(i)}-h_{\\theta}(x^{(i)})x_j^{(i)},(\\text{for every j})\\\\\n",
    "& \\,\\text{\\}} \\\\\n",
    "& \\text{\\}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小批量梯度下降法MBGD\n",
    "小批量梯度下降法是指，不采样批量梯度下降一次迭代总样本量$m$，也不使用随机梯度下降的每次只迭代一组样本的串行运算，而是使用$b$组样本进行迭代，以获得权值$\\theta$的更新。  \n",
    "只需要使用批量梯度下降法的公式，把$m$换成$b$即可"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
